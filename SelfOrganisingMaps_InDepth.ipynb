{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data will be a collection of random colours, so first we’ll artificially create a dataset of 100. Each colour is a 3D vector representing R, G and B values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255.  ,   0.  ,   0.  ,   0.  ,   0.  , 255.  , 255.  , 255.  ],\n",
       "       [  0.  , 255.  , 127.5 ,   0.  ,   0.  , 255.  , 102.  ,   0.  ],\n",
       "       [  0.  ,   0.  ,  63.75, 255.  , 127.5 ,  51.  ,  63.75, 255.  ]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 colours as initial test set\n",
    "raw_data = np.array([[1, 0, 0], [0, 1, 0],\n",
    "                     [0, 0.5, 0.25], [0, 0, 1],\n",
    "                     [0, 0, 0.5], [1, 1, 0.2],\n",
    "                     [1, 0.4, 0.25], [1, 0, 1]]).T * 255\n",
    "# or use random colours\n",
    "\n",
    "#raw_data = np.random.randint(0, 255, (3, 100))\n",
    "\n",
    "#That’s simply 100 rows of 3D vectors all between the values of 0 and 255.\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, here’s what we’re trying to do. We want to take our 3D colour vectors and map them onto a 2D surface in such a way that similar colours will end up in the same area of the 2D surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important aspect of the SOM is that each of the 2D points on the grid actually represent a multi-dimensional weight vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point on the SOM has a weight vector associated with it that is the same number of dimensions as our input data, in this case 3 to match the 3 dimensions of our colours. \n",
    "We’ll see why this is important when we go through the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The learning rate decides by how much we apply changes to our SOM at each iteration.\n",
    " \n",
    "In practice it is best to start with a larger learning rate and reduce it slowly over time. This is so that the SOM can start by making big changes but then settle into a solution after a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this post I will use 3D to refer to the dimensionality of the input data (which in reality could be any number of dimensions) and 2D as the dimensionality of the SOM (which we decide and could also be any number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup the SOM we need to start with the following:\n",
    "\n",
    "Decide on and initialise the SOM parameters (as above)\n",
    "\n",
    "Setup the grid by creating a 5×5 array of random 3D weight vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dimensions = np.array([5, 5])\n",
    "n_iterations = 10000\n",
    "init_learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish variables based on data\n",
    "m = raw_data.shape[0]\n",
    "n = raw_data.shape[1]\n",
    "\n",
    "# initial neighbourhood radius\n",
    "init_radius = max(network_dimensions[0], network_dimensions[1]) / 2\n",
    "\n",
    "# setup random weights between 0 and 1\n",
    "# weight matrix needs to be one m-dimensional vector for each neuron in the SOM\n",
    "net = np.random.random((network_dimensions[0], network_dimensions[1], m))\n",
    "\n",
    "# radius decay parameter\n",
    "time_constant = n_iterations / np.log(init_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.99999326e-01, 1.45545065e-02, 9.08599930e-03],\n",
       "        [9.99999987e-01, 1.92468517e-01, 1.20029969e-01],\n",
       "        [9.99110674e-01, 4.08344135e-01, 2.48821616e-01],\n",
       "        [4.77130873e-01, 7.15960930e-01, 1.19102699e-01],\n",
       "        [2.78532303e-04, 9.98951359e-01, 5.98140515e-04]],\n",
       "\n",
       "       [[9.99999143e-01, 3.59814030e-02, 2.26888584e-02],\n",
       "        [9.99999549e-01, 4.67170744e-01, 1.61789387e-01],\n",
       "        [9.99976537e-01, 7.03086916e-01, 2.24737903e-01],\n",
       "        [4.51636909e-01, 7.66213328e-01, 1.65906446e-01],\n",
       "        [6.27196534e-05, 7.51678167e-01, 1.24173633e-01]],\n",
       "\n",
       "       [[9.99989255e-01, 2.90761529e-02, 9.44074608e-01],\n",
       "        [9.99994960e-01, 8.96064041e-01, 2.61464934e-01],\n",
       "        [9.98648219e-01, 9.79890051e-01, 2.02125050e-01],\n",
       "        [4.96129834e-01, 7.48069637e-01, 2.25192856e-01],\n",
       "        [2.55666964e-06, 5.00037580e-01, 2.49996606e-01]],\n",
       "\n",
       "       [[9.99624159e-01, 3.68708266e-04, 9.99244802e-01],\n",
       "        [9.34355942e-01, 7.07140928e-02, 9.43375126e-01],\n",
       "        [5.07927962e-01, 5.07650380e-01, 5.93860403e-01],\n",
       "        [2.60974390e-01, 3.83838011e-01, 4.76024800e-01],\n",
       "        [1.94720500e-07, 2.45680900e-01, 3.77162200e-01]],\n",
       "\n",
       "       [[9.93397082e-01, 1.95565092e-04, 9.99719103e-01],\n",
       "        [1.03704831e-01, 1.19724195e-05, 9.99948982e-01],\n",
       "        [1.85768832e-04, 1.98528897e-05, 9.98832580e-01],\n",
       "        [9.69205288e-07, 3.08410332e-06, 7.39412066e-01],\n",
       "        [5.11067154e-06, 7.96344812e-04, 5.00309335e-01]]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, SOMs train faster (and “better”) if all our values are between 0 and 1. \n",
    "\n",
    "This is often true with machine learning problems, and it’s to avoid one of our dimensions “dominating” the others in the learning process. \n",
    "\n",
    "For example, if one of our variable was salary (in the thousands) and another was height (in metres, so rarely over 2.0) then salary will get a higher importance simply because it has much higher values. \n",
    "\n",
    "Normalising to the unit interval will remove this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_data = True\n",
    "\n",
    "# if True, assume all data is on common scale\n",
    "# if False, normalise to [0 1] range along each column\n",
    "normalise_by_column = False\n",
    "\n",
    "# we want to keep a copy of the raw data for later\n",
    "data = raw_data\n",
    "\n",
    "# check if data needs to be normalised\n",
    "if normalise_data:\n",
    "    if normalise_by_column:\n",
    "        # normalise along each column\n",
    "        col_maxes = raw_data.max(axis=0)\n",
    "        data = raw_data / col_maxes[np.newaxis, :]\n",
    "    else:\n",
    "        # normalise entire dataset\n",
    "        data = raw_data / data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select a Random Input Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a training example at random\n",
    "t = data[:, np.random.randint(0, n)].reshape(np.array([m, 1]))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the Best Matching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find its Best Matching Unit\n",
    "bmu, bmu_idx = find_bmu(t, net, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that to work we need a function to find the BMU. \n",
    "\n",
    "It need to iterate through each neuron in the SOM, measure its Euclidean distance to our input vector and return the one that’s closest. \n",
    "\n",
    "Note the implementation trick of not actually measuring Euclidean distance, but the squared Euclidean distance, thereby avoiding an expensive square root computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bmu(t, net, m):\n",
    "    \"\"\"\n",
    "        Find the best matching unit for a given vector, t, in the SOM\n",
    "        Returns: a (bmu, bmu_idx) tuple where bmu is the high-dimensional BMU\n",
    "                 and bmu_idx is the index of this vector in the SOM\n",
    "    \"\"\"\n",
    "    bmu_idx = np.array([0, 0])\n",
    "    # set the initial minimum distance to a huge number\n",
    "\n",
    "    min_dist = np.iinfo(np.int).max\n",
    "    # calculate the high-dimensional distance between each neuron and the input\n",
    "\n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :].reshape(m, 1)\n",
    "\n",
    "            # don't bother with actual Euclidean distance, to avoid expensive sqrt operation\n",
    "            sq_dist = np.sum((w - t) ** 2)\n",
    "\n",
    "            if sq_dist < min_dist:\n",
    "                min_dist = sq_dist\n",
    "                bmu_idx = np.array([x, y])\n",
    "\n",
    "    # get vector corresponding to bmu_idx\n",
    "    bmu = net[bmu_idx[0], bmu_idx[1], :].reshape(m, 1)\n",
    "    # return the (bmu, bmu_idx) tuple\n",
    "    return (bmu, bmu_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Update the SOM Learning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, we want to decay the learning rate over time to let the SOM “settle” on a solution.\n",
    "\n",
    "What we also decay is the neighbourhood radius, which defines how far we search for 2D neighbours when updating vectors in the SOM. \n",
    "\n",
    "We want to gradually reduce this over time, like the learning rate. We’ll see this in a bit more detail in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_radius(initial_radius, i, time_constant):\n",
    "    return initial_radius * np.exp(-i / time_constant)\n",
    "\n",
    "def decay_learning_rate(initial_learning_rate, i, n_iterations):\n",
    "    return initial_learning_rate * np.exp(-i / n_iterations)\n",
    "\n",
    "def calculate_influence(distance, radius):\n",
    "    return np.exp(-distance / (2* (radius**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOM Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_iterations):\n",
    "    \n",
    "    # select a training example at random\n",
    "    t = data[:, np.random.randint(0, n)].reshape(np.array([m, 1]))\n",
    "    \n",
    "    # find its Best Matching Unit\n",
    "    bmu, bmu_idx = find_bmu(t, net, m)\n",
    "    \n",
    "    # decay the SOM parameters\n",
    "    r = decay_radius(init_radius, i, time_constant)\n",
    "    l = decay_learning_rate(init_learning_rate, i, n_iterations)\n",
    "    #print('Iteration %d, radius %d, learning rate %d' %(i, r, l))\n",
    "    \n",
    "    \n",
    "    # now we know the BMU, update its weight vector to move closer to input\n",
    "    # and move its neighbours in 2-D space closer\n",
    "    # by a factor proportional to their 2-D distance from the BMU\n",
    "    for x in range(net.shape[0]):\n",
    "        for y in range(net.shape[1]):\n",
    "            w = net[x, y, :].reshape(m, 1)\n",
    "    \n",
    "            # get the 2-D distance (again, not the actual Euclidean distance)\n",
    "            w_dist = np.sum((np.array([x, y]) - bmu_idx) ** 2)\n",
    "\n",
    "            # if the distance is within the current neighbourhood radius\n",
    "            if w_dist <= r**2:\n",
    "                # calculate the degree of influence (based on the 2-D distance)\n",
    "                influence = calculate_influence(w_dist, r)\n",
    "\n",
    "                # now update the neuron's weight using the formula:\n",
    "                # new w = old w + (learning rate * influence * delta)\n",
    "                # where delta = input vector (t) - old w\n",
    "                new_w = w + (l * influence * (t - w))\n",
    "                # commit the new weight\n",
    "                net[x, y, :] = new_w.reshape(1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Colour Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "# setup axes\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "ax.set_xlim((0, net.shape[0]+1))\n",
    "ax.set_ylim((0, net.shape[1]+1))\n",
    "ax.set_title('Self-Organising Map after %d iterations' % n_iterations)\n",
    "\n",
    "# plot the rectangles\n",
    "for x in range(1, net.shape[0] + 1):\n",
    "    for y in range(1, net.shape[1] + 1):\n",
    "        ax.add_patch(patches.Rectangle((x-0.5, y-0.5), 1, 1,\n",
    "                     facecolor=net[x-1,y-1,:],\n",
    "                     edgecolor='none'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
